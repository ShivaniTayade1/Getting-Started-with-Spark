{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnt81spJo79f"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Homework2_Spark_Chapter2\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "\n",
        "# Load all the summary CSV files (2010-2015)\n",
        "df = spark.read.csv(\"gs://msba-tasmania-svtayade/201*-summary.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Explore schema and data\n",
        "df.printSchema()\n",
        "df.show(5)\n",
        "df.describe().show()\n",
        "\n",
        "# Ensure 'count' is numeric\n",
        "df = df.withColumn(\"count\", F.col(\"count\").cast(\"int\"))\n",
        "\n",
        "# Basic analysis\n",
        "df.groupBy(\"DEST_COUNTRY_NAME\").count().orderBy(F.desc(\"count\")).show(10)\n",
        "\n",
        "# Add year from file name\n",
        "df_with_year = df.withColumn(\"source_file\", F.input_file_name())\n",
        "df_with_year = df_with_year.withColumn(\"year\", F.regexp_extract(\"source_file\", r'(\\d{4})', 1))\n",
        "\n",
        "df_with_year.select(\"year\", \"DEST_COUNTRY_NAME\", \"count\").show(5)\n",
        "\n",
        "# Group by year and country\n",
        "df_with_year.groupBy(\"year\", \"DEST_COUNTRY_NAME\") \\\n",
        "    .sum(\"count\") \\\n",
        "    .orderBy(\"year\", F.desc(\"sum(count)\")) \\\n",
        "    .show()\n",
        "\n",
        "# Load temperature data\n",
        "df_temp = spark.read.csv(\n",
        "    \"gs://msba-tasmania-svtayade/GlobalLandTemperatures_GlobalLandTemperaturesByCountry.csv\",\n",
        "    header=True, inferSchema=True\n",
        ")\n",
        "\n",
        "# Extract year\n",
        "df_temp = df_temp.withColumn(\"Year\", F.year(\"dt\"))\n",
        "df_temp.createOrReplaceTempView(\"temp\")\n",
        "\n",
        "# a. Highest average temperature\n",
        "spark.sql(\"\"\"\n",
        "    SELECT Country, Year, AVG(AverageTemperature) as AvgTemp\n",
        "    FROM temp\n",
        "    GROUP BY Country, Year\n",
        "    ORDER BY AvgTemp DESC\n",
        "    LIMIT 1\n",
        "\"\"\").show()\n",
        "\n",
        "# b. Top 10 countries with highest temp change\n",
        "df_country_year = spark.sql(\"\"\"\n",
        "    SELECT Country, Year, AVG(AverageTemperature) as AvgTemp\n",
        "    FROM temp\n",
        "    GROUP BY Country, Year\n",
        "\"\"\")\n",
        "\n",
        "w_first = Window.partitionBy(\"Country\").orderBy(\"Year\")\n",
        "w_last  = Window.partitionBy(\"Country\").orderBy(F.desc(\"Year\"))\n",
        "\n",
        "first_last = (\n",
        "    df_country_year\n",
        "    .withColumn(\"FirstYear\",     F.first(\"Year\").over(w_first))\n",
        "    .withColumn(\"FirstYearTemp\", F.first(\"AvgTemp\").over(w_first))\n",
        "    .withColumn(\"LastYear\",      F.first(\"Year\").over(w_last))\n",
        "    .withColumn(\"LastYearTemp\",  F.first(\"AvgTemp\").over(w_last))\n",
        "    .select(\"Country\", \"FirstYear\", \"FirstYearTemp\", \"LastYear\", \"LastYearTemp\")\n",
        "    .distinct()\n",
        "    .withColumn(\"TempChange\", F.col(\"LastYearTemp\") - F.col(\"FirstYearTemp\"))\n",
        ")\n",
        "\n",
        "first_last.orderBy(F.desc(\"TempChange\")).show(10, truncate=False)\n",
        "\n",
        "# Load CO2 data\n",
        "co2_path = \"gs://msba-tasmania-svtayade/CO2 emissions per capita per country.csv\"\n",
        "\n",
        "co2_raw = (\n",
        "    spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(co2_path)\n",
        ")\n",
        "\n",
        "# Convert wide to long format\n",
        "years = [str(y) for y in range(1960, 2015)]\n",
        "expr = (\n",
        "    \"stack(\" + str(len(years)) + \", \" +\n",
        "    \", \".join([f\"'{y}', `{y}`\" for y in years]) +\n",
        "    \") as (Year, CO2_per_capita)\"\n",
        ")\n",
        "\n",
        "co2_long = (\n",
        "    co2_raw.selectExpr(\"`Country Name` as Country\", expr)\n",
        "           .filter(F.col(\"CO2_per_capita\").isNotNull())\n",
        "           .withColumn(\"Year\", F.col(\"Year\").cast(\"int\"))\n",
        ")\n",
        "\n",
        "# Filter temperature data for same range\n",
        "temp_60_14 = (\n",
        "    df_country_year.filter((F.col(\"Year\") >= 1960) & (F.col(\"Year\") <= 2014))\n",
        ")\n",
        "\n",
        "# Join datasets\n",
        "merged = (\n",
        "    temp_60_14.join(co2_long, on=[\"Country\", \"Year\"])\n",
        "              .select(\"Country\", \"Year\", \"AvgTemp\", \"CO2_per_capita\")\n",
        ")\n",
        "\n",
        "print(f\"Rows after merge (1960â€“2014): {merged.count()}\")\n",
        "merged.show(5, truncate=False)\n",
        "\n",
        "# Global correlation\n",
        "corr_val = merged.stat.corr(\"AvgTemp\", \"CO2_per_capita\")\n",
        "print(f\"Pearson correlation (AvgTemp vs CO2 per capita): {corr_val:.4f}\")\n",
        "\n",
        "# Correlation by country\n",
        "corr_by_country = (\n",
        "    merged.groupBy(\"Country\")\n",
        "          .agg(F.corr(\"AvgTemp\", \"CO2_per_capita\").alias(\"Corr\"))\n",
        "          .orderBy(F.desc(\"Corr\"))\n",
        ")\n",
        "\n",
        "corr_by_country.show(5, truncate=False)\n",
        "\n",
        "# End of script\n",
        "spark.stop()\n"
      ]
    }
  ]
}